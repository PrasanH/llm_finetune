{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6fbcc31-7204-4b79-874a-f23300aa42d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch                                  # Pytorch\n",
    "!pip install -q transformers datasets                  # Comes from HuggingFace\n",
    "!pip install -q bitsandbytes                           # For quantization from HuggingFace\n",
    "!pip install -q peft                                   # Parameter-efficient Fine-tuning from HuggingFace\n",
    "!pip install -q trl                                    # For supervised fine-tuning for LLMs from HuggingFace\n",
    "!pip install -q accelerate        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59acf04d-7ede-420c-a1ad-38aab4cbd75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "my_hf_key= os.getenv('HF_KEY')\n",
    "\n",
    "login(token= my_hf_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e640fdc-f60c-4382-813c-0e3be455d8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2800, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_C = pd.read_csv(\"deliverables/df_csv/df_C.csv\", encoding='utf-8')\n",
    "df_C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e14399b-f479-424d-9c79-60f441a232d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>German</th>\n",
       "      <th>French</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Meine Freundin arbeitet im Büro und ich besuch...</td>\n",
       "      <td>Ma petite amie travaille au bureau et je la vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Herr Direktor wunscht Ihnen alles Gute und sch...</td>\n",
       "      <td>Le directeur vous souhaite une santé de fer et...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Das Internet bietet viele Möglichkeiten, um si...</td>\n",
       "      <td>Internet offre de nombreuses possibilités pour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mein Chef ist sehr anspruchsvoll, aber ich res...</td>\n",
       "      <td>Mon patron est très exigeant, mais je respecte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Das Lesen von Büchern über verschiedene Kultur...</td>\n",
       "      <td>La lecture de livres sur différentes cultures ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              German  \\\n",
       "0  Meine Freundin arbeitet im Büro und ich besuch...   \n",
       "1  Herr Direktor wunscht Ihnen alles Gute und sch...   \n",
       "2  Das Internet bietet viele Möglichkeiten, um si...   \n",
       "3  Mein Chef ist sehr anspruchsvoll, aber ich res...   \n",
       "4  Das Lesen von Büchern über verschiedene Kultur...   \n",
       "\n",
       "                                              French  \n",
       "0  Ma petite amie travaille au bureau et je la vi...  \n",
       "1  Le directeur vous souhaite une santé de fer et...  \n",
       "2  Internet offre de nombreuses possibilités pour...  \n",
       "3  Mon patron est très exigeant, mais je respecte...  \n",
       "4  La lecture de livres sur différentes cultures ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_C.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795438da-277e-4060-a867-a1b011653183",
   "metadata": {},
   "source": [
    "We will use all these examples for training. But we will split 5% of them for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "752b4b8d-c730-4a8b-9747-ce423756c163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items in training and validation  are: 2520 and  280\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, validation_data = train_test_split(df_C, test_size=0.1, random_state=42)\n",
    "print(f\"items in training and validation  are: {len(train_data)} and  {len(validation_data)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f70aab1-b626-4e81-8f4c-73bdb8950af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert the DataFrames into a Dataset\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "validation_dataset = Dataset.from_pandas(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f78f5d7-f561-4160-bea2-e71cfb9aa9e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['German', 'French', '__index_level_0__'],\n",
       "    num_rows: 2520\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0e12b73-809f-4bbb-a37e-ed89fda1a9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(german_input, french_trans):\n",
    "    return f\"\"\"### Instruction:Translate to French.\n",
    "### Input German Sentence:\n",
    "{german_input.strip()}\n",
    "### French Translation:\n",
    "{french_trans.strip()}<|endoftext|>\"\"\" \n",
    "#EOS token for Qwen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2a25fbf-bf49-4bc6-a343-212588945beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that converts a dataset row into the corresponding prompt format\n",
    "def convert_to_instruction_format(data_point):\n",
    "    return {\n",
    "        \"text\": format_instruction(data_point[\"German\"], data_point[\"French\"])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd30e822-25fa-4ec9-a505-bab290dc1538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each row in the dataset\n",
    "def process_dataset(data):\n",
    "    return data.map(\n",
    "        convert_to_instruction_format\n",
    "        ).remove_columns(['German', 'French', '__index_level_0__']) #removing unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b5cccee-4978-4cd0-b30a-4046c1d56498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fbfdca9dda543fbb5febce199dbaeca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2520 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "943c4edfd3904f18af832d8c1a5fc6d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/280 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = process_dataset(train_dataset)\n",
    "validation_data = process_dataset(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f84c7f1-db4a-44b7-8a30-f5f9148a4460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2520\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117a8504-e958-4839-b2ae-c818420a0029",
   "metadata": {},
   "source": [
    "#### Load the model and finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "502a8576-f647-4944-998f-d440fe41f53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "                                             device_map = \"auto\",torch_dtype=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "629d6a4f-4046-4808-872b-44d2fccc112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the padding token to be the same as the end-of-sequence token.\n",
    "\n",
    "# Padding ensures all sequences in a batch are of equal length.\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Specify that padding should be applied to the right side of the sequences.\n",
    "# This is the standard behavior for causal language models. Causal models, predict the next token in a sequence using only the preceding tokens.\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0d4e626-32ff-4a96-a4f2-4d0c40b374c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary PEFT objects for preparing the model for LoRA training\n",
    "from peft import  LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "\n",
    "#model = prepare_model_for_kbit_training(model)  \n",
    "#Use only if loading a quantized model-includes float conversions to help stabilize training\n",
    "\n",
    "# Set up LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                                                       # The rank (dimensions) of the LoRA matrices A and B\n",
    "    lora_alpha=64,                                              # Scales the product of matrices AB [W_new = W_old + (A * B) * α]\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],    # Apply LoRA to the attention matrices\n",
    "    lora_dropout=0.1,                                           # Dropout rate to reduce overfitting\n",
    "    bias=\"none\",                                                # Do not train the bias parameter\n",
    "    task_type=\"CAUSAL_LM\"                                       # Task type for autoregressive text generation\n",
    ")\n",
    "\n",
    "# Get the model with unfrozen LoRA layers applied\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "385693c4-fae0-446b-86b1-8e149200832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "# Set up the training hyperparameters\n",
    "training_arguments = SFTConfig(\n",
    "    fp16=True,                           # Use 16-bit precision for training computations (optimizer states, gradients)\n",
    "    dataset_text_field=\"text\",           # Specify the text field in the dataset for training\n",
    "    max_seq_length=128,                 # Set the maximum sequence length for the training data\n",
    "\n",
    "    # Batch-related parameters\n",
    "    per_device_train_batch_size=4,       # Batch size per device during training\n",
    "\n",
    "    # Optimizer-related parameters\n",
    "    optim=\"paged_adamw_32bit\",           # Use the paged AdamW optimizer, optimized for 32-bit GPUs\n",
    "    learning_rate=1e-4,                  # Set the learning rate for training\n",
    "\n",
    "    # Epochs and saving configuration\n",
    "    num_train_epochs=4,                  # Number of training epochs (more epochs generally lead to better results)\n",
    "    save_strategy=\"epoch\",               # Save the model after each epoch\n",
    "    output_dir=\"./epoch-finetuned\",      # Directory to save the fine-tuned model\n",
    "\n",
    "    # Validation-related parameters\n",
    "    eval_strategy=\"steps\",               # Evaluation strategy, performed at specified steps\n",
    "    eval_steps=0.2,                      # Evaluate after 20% of the training steps\n",
    "\n",
    "    # Logging-related parameters\n",
    "    report_to=\"none\",                    # Disable reporting to external tools\n",
    "    logging_dir=\"./logs\",                # Directory to save the training logs\n",
    "    logging_steps=20,                    # Number of steps between each log entry\n",
    "    seed=42,                             # Set a random seed for reproducibility\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing to save memory and recompute during backpropagation\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Disable attention cache during training; it should be enabled during inference\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc78f7d0-2d21-49ac-8e40-9d931d3a7e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_75540/2900952727.py:5: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b6d8f3fb0c4c118eaca0c5174a42e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/2520 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8a9efe3f4040bfabb6d3edbc3b446e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/2520 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21e2852cba04272b1a096cda6e2ed13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/2520 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26da6830d3944239a87575cff76ed9dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/2520 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e757e8fa43f4e92a31606929b9d96b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting eval dataset to ChatML:   0%|          | 0/280 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a14168bad55c4ed380281795e4b7d700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/280 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9412ee9c31d403b9120c5345b2b929c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/280 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13213d851c2542039b90c955009636eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/280 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# Import the SFTTrainer from HuggingFace TRL library\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = SFTTrainer(\n",
    "    # Assign the model and tokenizer\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    "    # Provide the training and validation datasets\n",
    "    train_dataset= train_data,\n",
    "    eval_dataset=validation_data,\n",
    "\n",
    "    # Pass the LoRA configuration\n",
    "    peft_config=lora_config,\n",
    "\n",
    "    # Set the training hyperparameters\n",
    "    args=training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65c2806e-7b86-4369-a6c6-ee68f5abca00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2520' max='2520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2520/2520 10:51, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>504</td>\n",
       "      <td>0.935200</td>\n",
       "      <td>0.978554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1008</td>\n",
       "      <td>0.908200</td>\n",
       "      <td>0.947416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1512</td>\n",
       "      <td>0.798000</td>\n",
       "      <td>0.928107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016</td>\n",
       "      <td>0.803600</td>\n",
       "      <td>0.923929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>0.756800</td>\n",
       "      <td>0.919810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2520, training_loss=0.8796610998728919, metrics={'train_runtime': 652.9848, 'train_samples_per_second': 15.437, 'train_steps_per_second': 3.859, 'total_flos': 4769581015388160.0, 'train_loss': 0.8796610998728919})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start the training process \n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1490bcc9-7144-4cd7-a02e-ac616e72051e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 24M\n",
      "-rwxr--r-- 1 prasanna99h prasanna99h 5.0K Mar 12 15:36 README.md\n",
      "-rwxr--r-- 1 prasanna99h prasanna99h  722 Mar 12 15:36 adapter_config.json\n",
      "-rwxr--r-- 1 prasanna99h prasanna99h 8.4M Mar 12 15:36 adapter_model.safetensors\n",
      "-rwxr--r-- 1 prasanna99h prasanna99h  605 Mar 12 15:36 added_tokens.json\n",
      "-rwxr--r-- 1 prasanna99h prasanna99h 1.6M Mar 12 15:36 merges.txt\n",
      "-rwxr--r-- 1 prasanna99h prasanna99h  496 Mar 12 15:36 special_tokens_map.json\n",
      "-rwxr--r-- 1 prasanna99h prasanna99h  11M Mar 12 15:36 tokenizer.json\n",
      "-rwxr--r-- 1 prasanna99h prasanna99h 7.2K Mar 12 15:36 tokenizer_config.json\n",
      "-rwxr--r-- 1 prasanna99h prasanna99h 2.7M Mar 12 15:36 vocab.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Define the save path for the fine-tuned model on Colab\n",
    "peft_model_path = \"./fine-tuned-qwen2.5-1.5b-instruct-step10\"\n",
    "\n",
    "# Save the trained model\n",
    "trainer.model.save_pretrained(peft_model_path)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(peft_model_path)\n",
    "\n",
    "# List the saved files\n",
    "!ls -lh {peft_model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cb594f-f2fb-4108-95c4-549302aa77b4",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "#### Before loading the tuned model, lets move back the original model to cpu. We can also delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0119f75c-89e2-4854-990c-1fc42043c9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model.to(\"cpu\")  # Move the existing model to CPU\n",
    "\n",
    "torch.cuda.empty_cache()  # Clear GPU cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82e7a7b8-3df1-4139-ba1e-6003bf6a30e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loading a PEFT model, we need to use a special object for CausalLM from PEFT\n",
    "# instead of the regular HuggingFace object.\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the fine-tuned model\n",
    "peft_model_path = \"./fine-tuned-qwen2.5-1.5b-instruct-step10\"\n",
    "tuned_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    peft_model_path,\n",
    "      # Load with 4-bit quantization\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tuned_model.to(device)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_path)\n",
    "\n",
    "# Set the padding token to be the same as the end-of-sequence token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Specify that padding should be added to the right side of the sequences\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Enable attention cache during inference\n",
    "tuned_model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50671a0-ac4f-4cd5-9422-5d39312159ad",
   "metadata": {},
   "source": [
    "Now we load dataset A and generate the test set using same random seed as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16319b61-9eb7-4052-a062-285cc743c343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items in test test are:200\n"
     ]
    }
   ],
   "source": [
    "test_A = pd.read_csv(\"deliverables/df_csv/test_A.csv\", encoding='utf-8')\n",
    "print(f\"items in test test are:{len(test_A)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7876e63d-30d8-4405-bb8a-fc5a3303b52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_tuned_model(prompt):\n",
    "  inputs = tokenizer (prompt, return_tensors=\"pt\")\n",
    "  inputs = inputs.to(tuned_model.device)\n",
    "\n",
    "  output_tokens = tuned_model.generate(\n",
    "    inputs.input_ids,\n",
    "    max_new_tokens=50,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    ")[0]\n",
    "  output = tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00d0e402-9de4-4807-9933-cc8c611cce6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: 100%|██████████| 200/200 [06:39<00:00,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collected translations for 200 items.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "translation_list=[]\n",
    "for row in tqdm(range(len(test_A)),desc=\"Translating\"):\n",
    "\n",
    "  my_prompt = f\"\"\"German Sentence:{test_A.iloc[row, 0].strip()}.\n",
    "French Translation:\n",
    "\"\"\"\n",
    "  translation = get_output_tuned_model(my_prompt)\n",
    "  translation = translation.replace(my_prompt, \"\")\n",
    "  translation_list.append(translation)\n",
    "print(f'collected translations for {len(translation_list)} items.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72f0ece0-1334-48c4-8dbb-361365cfef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('translations_test_C_qwen.txt', 'w') as f:\n",
    "    for item in translation_list:\n",
    "        f.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c12c17f8-4bd2-4bcb-81b4-b91be11d7954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate         ##metric for evaulation, HF library\n",
    "def calculate_bleu_score(prediction, reference):\n",
    "\n",
    "\n",
    "  #reference = [[reference.lower().split()]]\n",
    "  #prediction = [prediction.lower().split()]\n",
    "  prediction = [prediction]\n",
    "  reference = [[reference]]\n",
    "\n",
    "  bleu = evaluate.load(\"bleu\")\n",
    "  score = bleu.compute(predictions= prediction,references=reference)\n",
    "\n",
    "  return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5992485-0aaf-48d1-86e1-b560ae9ae22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_evaluation_df(translation_list, actual_df):\n",
    "  \"\"\"\n",
    "  translation_list is a list of translations (has just the target language sentences) from the llm to be evaluated\n",
    "  actual_df : a dataframe containing the actual sentence and its translation\n",
    "  \"\"\"\n",
    "  evaluation =[]\n",
    "  evaluation_df = pd.DataFrame(columns=[\n",
    "    'Actual', \n",
    "    'Reference', \n",
    "    'Prediction', \n",
    "    'BLEU Score', \n",
    "    'Precisions', \n",
    "    'Brevity Penalty', \n",
    "    'Length Ratio', \n",
    "    'Translation Length', \n",
    "    'Reference Length'\n",
    "])\n",
    "  for i in tqdm(range(len(translation_list))):\n",
    "    actual = actual_df.iloc[i,0]\n",
    "    reference = actual_df.iloc[i,1]\n",
    "    prediction = translation_list[i]\n",
    "    score = calculate_bleu_score(prediction, reference)\n",
    "\n",
    "    evaluation_data = {\n",
    "        'Actual': actual,\n",
    "        'Reference': reference,\n",
    "        'Prediction': prediction,\n",
    "        'BLEU Score': score['bleu'],\n",
    "        'Precisions': score['precisions'],\n",
    "        'Brevity Penalty': score['brevity_penalty'],\n",
    "        'Length Ratio': score['length_ratio'],\n",
    "        'Translation Length': score['translation_length'],\n",
    "        'Reference Length': score['reference_length']\n",
    "    }\n",
    "    evaluation.append(evaluation_data)\n",
    "    \n",
    "    # Concatenate all evaluation data into the DataFrame\n",
    "    evaluation_df_concat = pd.concat([evaluation_df, pd.DataFrame(evaluation)], ignore_index=True)\n",
    "\n",
    "  return evaluation_df_concat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ecab6c2-bf81-4bdd-84fa-4c1634564815",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/tmp/ipykernel_75540/2784772388.py:40: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  evaluation_df_concat = pd.concat([evaluation_df, pd.DataFrame(evaluation)], ignore_index=True)\n",
      "100%|██████████| 200/200 [01:31<00:00,  2.18it/s]\n"
     ]
    }
   ],
   "source": [
    "result_df = get_evaluation_df(translation_list, test_A )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb9f0bed-df0b-400c-bb75-67ff5a2bdcfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 9)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b71450d-6540-482d-9da4-881c552ccde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average bleu score is 0.0056682964741560996\n"
     ]
    }
   ],
   "source": [
    "print(f'average bleu score is {result_df[\"BLEU Score\"].mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29039bfd-4fd4-4642-834a-9b8582de625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('results_df_C_step10.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e524fe0-f8c2-4ad0-90b6-7e97ee5e873a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
